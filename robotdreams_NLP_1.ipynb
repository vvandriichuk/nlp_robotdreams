{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "cv = CountVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6 # only top-6 words will be used as columns\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TfidfVectorizer object\n",
    "tv = TfidfVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6, # only top-6 words will be used as columns,\n",
    "                    smooth_idf=True,\n",
    "                    norm='l2' # euclidean norm используется по дефолту\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', CountVectorizer()),\n",
    "#     ('clf', LogisticRegression()),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем перебор лучших параметров C для логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = dict(clf__C=[10, 1, 0.1, 0.01])\n",
    "# grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший параметр C = 1. Используем это в модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()), ('clf', LogisticRegression(C=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(C=1)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7915560276155071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.76      0.71      0.74       319\n",
      "           comp.graphics       0.67      0.75      0.71       389\n",
      " comp.os.ms-windows.misc       0.71      0.68      0.69       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.68      0.67       392\n",
      "   comp.sys.mac.hardware       0.78      0.79      0.79       385\n",
      "          comp.windows.x       0.81      0.70      0.75       395\n",
      "            misc.forsale       0.82      0.88      0.85       390\n",
      "               rec.autos       0.83      0.85      0.84       396\n",
      "         rec.motorcycles       0.91      0.90      0.90       398\n",
      "      rec.sport.baseball       0.84      0.88      0.86       397\n",
      "        rec.sport.hockey       0.93      0.91      0.92       399\n",
      "               sci.crypt       0.87      0.87      0.87       396\n",
      "         sci.electronics       0.69      0.71      0.70       393\n",
      "                 sci.med       0.82      0.74      0.77       396\n",
      "               sci.space       0.91      0.87      0.89       394\n",
      "  soc.religion.christian       0.87      0.89      0.88       398\n",
      "      talk.politics.guns       0.71      0.84      0.77       364\n",
      "   talk.politics.mideast       0.88      0.88      0.88       376\n",
      "      talk.politics.misc       0.70      0.55      0.62       310\n",
      "      talk.religion.misc       0.60      0.64      0.62       251\n",
      "\n",
      "                accuracy                           0.79      7532\n",
      "               macro avg       0.79      0.79      0.79      7532\n",
      "            weighted avg       0.79      0.79      0.79      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression + TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', TfidfVectorizer()),\n",
    "#     ('clf', LogisticRegression()),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем такой же перебор лучших параметров C для логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'clf__penalty': ['l1','l2'], 'clf__C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "# grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший параметр C = 1000. Используем это в модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', TfidfVectorizer()),\n",
       "                ('clf', LogisticRegression(C=1000))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(C=1000)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441317047265002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.78      0.79       319\n",
      "           comp.graphics       0.75      0.79      0.77       389\n",
      " comp.os.ms-windows.misc       0.75      0.71      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.69      0.71      0.70       392\n",
      "   comp.sys.mac.hardware       0.82      0.85      0.84       385\n",
      "          comp.windows.x       0.85      0.76      0.81       395\n",
      "            misc.forsale       0.83      0.90      0.86       390\n",
      "               rec.autos       0.92      0.90      0.91       396\n",
      "         rec.motorcycles       0.96      0.96      0.96       398\n",
      "      rec.sport.baseball       0.93      0.94      0.94       397\n",
      "        rec.sport.hockey       0.97      0.98      0.97       399\n",
      "               sci.crypt       0.94      0.93      0.93       396\n",
      "         sci.electronics       0.78      0.79      0.79       393\n",
      "                 sci.med       0.91      0.84      0.88       396\n",
      "               sci.space       0.92      0.94      0.93       394\n",
      "  soc.religion.christian       0.84      0.93      0.88       398\n",
      "      talk.politics.guns       0.74      0.90      0.81       364\n",
      "   talk.politics.mideast       0.95      0.88      0.92       376\n",
      "      talk.politics.misc       0.79      0.61      0.68       310\n",
      "      talk.religion.misc       0.70      0.61      0.65       251\n",
      "\n",
      "                accuracy                           0.84      7532\n",
      "               macro avg       0.84      0.84      0.84      7532\n",
      "            weighted avg       0.85      0.84      0.84      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', CountVectorizer()),\n",
    "#     ('clf', XGBClassifier()),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = dict(clf__max_depth=[3, 4, 5, 6, 7, 8])\n",
    "# grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=3, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', XGBClassifier(max_depth=3)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841210833775889"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.77      0.67      0.72       319\n",
      "           comp.graphics       0.67      0.73      0.70       389\n",
      " comp.os.ms-windows.misc       0.76      0.74      0.75       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.74      0.69       392\n",
      "   comp.sys.mac.hardware       0.75      0.79      0.77       385\n",
      "          comp.windows.x       0.84      0.72      0.77       395\n",
      "            misc.forsale       0.80      0.88      0.84       390\n",
      "               rec.autos       0.85      0.80      0.83       396\n",
      "         rec.motorcycles       0.90      0.87      0.88       398\n",
      "      rec.sport.baseball       0.86      0.89      0.87       397\n",
      "        rec.sport.hockey       0.92      0.89      0.91       399\n",
      "               sci.crypt       0.90      0.86      0.88       396\n",
      "         sci.electronics       0.54      0.65      0.59       393\n",
      "                 sci.med       0.83      0.80      0.81       396\n",
      "               sci.space       0.88      0.90      0.89       394\n",
      "  soc.religion.christian       0.88      0.91      0.90       398\n",
      "      talk.politics.guns       0.66      0.80      0.73       364\n",
      "   talk.politics.mideast       0.96      0.74      0.84       376\n",
      "      talk.politics.misc       0.67      0.54      0.60       310\n",
      "      talk.religion.misc       0.62      0.63      0.63       251\n",
      "\n",
      "                accuracy                           0.78      7532\n",
      "               macro avg       0.79      0.78      0.78      7532\n",
      "            weighted avg       0.79      0.78      0.79      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.3.1)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.23.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.5)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (2.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', CountVectorizer()),\n",
    "#     ('clf', LGBMClassifier(min_data_in_leaf=100)),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова перебираем лучшие параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'clf__min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LGBMClassifier(min_data_in_leaf=100)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-783f9185256f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('bow', CountVectorizer()),\n",
    "#     ('clf', CatBoostClassifier()),\n",
    "# ])\n",
    "\n",
    "# params = {\n",
    "#     'clf__depth': [4, 6, 10],\n",
    "#     }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=1, verbose=1)\n",
    "\n",
    "# grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', CatBoostClassifier(depth=6)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
